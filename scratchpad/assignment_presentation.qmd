---
title: "Sailing the feature space with anchors"
subtitle: "BEX6510 Foundations of Econometrics"
author: "Janith Wanniarachchi"
format:
  revealjs:
    theme: [default, assignment_slide_styles.scss]
    slide-number: true
---

## Let's be honest here {.center}

. . . 

Building a good model is *hard*

. . . 

Explaining how a good model work is even **harder**

##  {.center .what-if}

. . . 

<img src="imgs/black_box_model-2.png" style = "width: 20%"/>
<br/>
<span style = "font-style: italic; font-size: 1.2rem"> Exhibit A: The good model </span>

. . . 

<div> **What if** you could <br/> poke around and find out <br/> how this model works

. . . 

**Introducing**

<div class = "accent-color"> **Explainable AI (XAI) methods!** </div>


## What is an <span class = "what-is-a-obj"> anchor </span>?

*Formally*

:::{.in-serif}

a rule or a set of predicates that satisfy the given instance and is a sufficient condition for $f(x)$ (i.e. the model output) with high probability

:::

<hr/>

:::{.callout-important}
## Simply put

We are going to find a big enough boundary box in the feature space containing other points that would have the same model prediction as the anchoring point.
:::

## What is a <span class = "what-is-a-obj"> predicate </span>?

Predicates are simple logical statements. In this context a predicate is made up of

:::: {.columns}
::: {.column width="20%"}
A feature 
<br/>

:::{.in-serif}
Age
:::

:::

::: {.column width="40%"}
A logical operator
<br/>

:::{.in-serif}
\>
:::

:::

::: {.column width="40%"}
A constant value
<br/>

:::{.in-serif}
42
:::

:::

::::

:::{.callout-important}
## Simply put

A predicate is a boundary line that divides a feature into two subsets.

:::

## What is a <span class = "what-is-a-obj"> coverage </span>?

*Formally,*

$$
\text{cov}(\mathcal{A}) = \mathbb{E}_{\mathcal{D}(z)}[\mathcal{A}(z)]
$$

:::{.callout-important}
## Simply put

The coverage of an anchor tells us how much of the total number of samples from the perturbation distribution does the anchor cover?

:::

## What is a <span class = "what-is-a-obj"> perturbation distribution </span>?

:::{.callout-important}
## Simply put

A method of generating varied versions of the data (kind of like alternate realities of the same data). 

:::

The simplest form of a perturbation distribution would be $\mathcal{N}_p(\underline{\mathbb{x}}, \Sigma)$

where $\underline{\mathbb{x}}$ is the local instance, and $\Sigma$ is the covariance matrix of the dataset.

## What is a <span class = "what-is-a-obj"> precision </span>?

*Formally*

$$
\text{Prec}(\mathcal{A}) = \mathbb{E}_{\mathcal{D}(z|\mathcal{A})}[\mathbb{1}_{f(x) = f(z)}]
$$

:::{.callout-important}
## Simply put
The precision of an anchor tells us how much of the samples from the perturbation distribution are in the same class as the given instance when applied to the model.
:::

## What is a <span class = "what-is-a-obj"> Multi-arm bandit problem </span>?

::: {.callout-important}
## Simply put
Multi-arm bandit problems are like playing different slot machines. You want to design a strategy to get the most rewards by choosing the best machines, even when you don't know how likely they are to pay out.
:::

## What is a <span class = "what-is-a-obj"> KL LUCB strategy </span>?

::: {.callout-important}
## Simply put
While trying to maximize the reward the agent will have to trade-off between either exploring the available choices or exploiting the known choices that give high rewards. KL LUCB is such a strategy that will purely explore the arms without exploiting.
:::

## What is a <span class = "what-is-a-obj"> Beam Search algorithm </span>?

::: {.callout-important}
## Simply put
Beam search is a search algorithm that looks at a few closest options at a time, picks the best based on a rule, and only keeps a certain number of them and throws away the rest.
:::

## <span style="text-decoration: line-through;color: lightgray;font-size: 3rem;"> What is a </span> How is an anchor?

The problem of finding the best anchors can be summarized in the following simple steps

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "|1|2|5|6|7-9|10-14"

A_0 <- anchor() # empty anchor
A_final <- NULL # initialize final anchor
A_t_1 <- A_0 # starting with t = 1
while(TRUE) {
  A_t <- GetAnchors(A_t_1, coverage(A_final))
  A_t <- BeamSearch_KL_LUCB(A_t, hyperparams) # hyper parameters of beam search
  if(is_empty(A_t)) { # beam search could not find any good anchors
    break # end loop and A_final is the final anchor
  }
  for(anchor in A_t) {
    if(precision(anchor) > threshold and coverage(anchor) > coverage(A_final)) {
      A_final <- anchor
    }
  }
}
``` 

# Homemade DIY anchors

That previous slide was confusing üòµ‚Äçüí´

. . . 

How about we try to make this by ourselves?

## But why?

### The motivation to provide this extension 

:::{.incremental}
* Deconstruncting and explaining tools in simpler terms encourages adoption of the tool.
* There is a gap on the material available to explain how XAI tools work internally in simple terms
* This could be one of the reasons that the adoption of XAI methods is becoming slower.
* Let's fix that! üßëüèª‚Äçüîß üë®üèª‚Äçüîß
:::

## Some minor changes {.smaller}

We are

::::{.columns}

:::{.column width="50%"}

:::{.fragment .semi-fade-out fragment-index=1}
* **NOT** going to calculate the coverage using perturbation distribution
:::

:::{.fragment .semi-fade-out fragment-index=3}
* **NOT** going to start with an empty anchor
:::

:::{.fragment .semi-fade-out fragment-index=7}
* **NOT** going to use the KL-LUCB algorithm
:::

:::

:::{.column width="50%"}

:::{.fragment .fade-in fragment-index=2}
* going to calculate coverage based on the fraction of the feature space that the anchor covers
:::

:::{.fragment .fade-in fragment-index=4}
* going to start with an anchor containing the point
:::

:::{.fragment .fade-in fragment-index=6}
* going to use a simpler algorithm for multi-arm bandit solution like UCB
:::

:::

::::

## Let's start with One Dimension

Given the following dataset of one dimension and a model trained on top of it, we want to find anchors.

<p style = "text-align:center"><img src = "imgs/1dim_here_plot.png" style = "width:60%"/></p>

## 

We can generate some bounding boxes around the given point

<p style = "text-align:center"><img src="imgs/1dim_sample_bounds.png" style="width:60%"/> </p>

## The brute force approach

*Simply put,* we try every single possible bounding box since there is a finite amount of bounding boxes.

<p style = "text-align:center"><img src="imgs/1dim_brute_results.png" style="width:60%"/></p>

# Let's get high  er dimensions into the story

One dimension? Easy peasy üçã squeezy

What about two?

## Two dimensional data

*The story is the same as before,*

<p style = "text-align:center"><img src="imgs/2dim_here_plot.png" style = "width:60%"/></p>

# Can we brute force this?

. . . 

Yes, but we will be waiting here for a long time

## Sequential Greedy Method

Let's take one thing at a time.

. . . 

Let's first find the best bounding box by changing the boundaries along one dimension only.

. . . 

Then we fix the boundary along that dimension and then we move the boundary box along the other dimension.

. . . 

Until we run out of dimensions... ü§∑üèª‚Äç‚ôÇÔ∏è

## Results

<p style = "text-align:center"><img src="imgs/2dim_seq_greed_results.png" style="width:60%"/></p>

. . . 

Not bad. But this took several minutes. Can we seach faster with a multi-arm bandit approach?

## Formulating the Multi-arm Bandit problem

:::{.callout-important}
## Simply put

To put it simply imagine you are the local point and you are trying to find similar friends like yourself by pushing a wall around you. Your options are to either push the north, east, west, or south walls to find new friends. When you increase the walls you get rewarded and when you find like minded friends you get rewarded as well.

:::

## What is our reward

*Formally* the reward can be defined as follows,

$$
\begin{equation}
R(\mathcal{A}) =
    \begin{cases}
        \text{Prec}(\mathcal{A}) + \text{cov}(\mathcal{A})^2 & \text{if } \text{Prec}(\mathcal{A}) \in \mathbb{R} \\
        -9999 & \text{if } \text{Prec}(\mathcal{A}) \notin \mathbb{R} \\
        -9999 & \text{if } \text{Prec}(\mathcal{A}) < 0.6 
    \end{cases}
\end{equation}
$$

:::{.callout-important}
## Simply put,
We are going to be rewarded more for covering more ground while also being precise, and we are going to punished if we push the walls into areas with weird precision values
:::

## Decisions, Decisions, Decisions

*Formally* based on the Upper Confidence Bound algorithm we select the action as follows,

$$
a = \underset{a}{\mathrm{argmax }} Q^*(a) + Q(a) + \sqrt{\frac{2 \cdot ln(g)}{N(a)}}
$$

:::{.callout-important}
## Simply put,
WHen thinking about a specific action We are going to consider the rewards we have received in previous games and the rewards we have received so far and also whether we have explored this action enough.
:::

## Final Results

![](assignment_state_plot_1.gif)

Looks cool. But it can definitely be more improved.

## Contributions {.smaller}

1.  Using the area covered within the feature space to compute the coverage instead of using a perturbation distribution. 
2. Using a top down approach instead of a bottom up approach to building anchors.
3. Using the UCB algorithm instead of the KL-LUCB algorithm to demonstrate the need for a pure exploration approach as the multi-arm bandit solution.
4. Providing the intuition of anchors in the following scenarios.
  1. A brute force approach in one dimension. 
  2. A sequentially greedy approach in two dimensions.
  3. A multi-arm bandit approach in two dimensions.
5. Implementing a pure R solution using novel data structures to ease debugging and encourage understanding of how anchors work.

# Questions?
